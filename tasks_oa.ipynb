{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\siddu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\siddu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "- I will be implementing a sentence transformer model using pytorch. \n",
    "- In real time, I would rather use a pre-trained model from huggingface because that would save a lot of time and would perform better than something that can be made locally. \n",
    "- For OA purposes, I will be making a very basic transformer from scratch using just pytorch. However, I will be using NLTK for testing the transformer.\n",
    "- Design choice will be below the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        Positional encoding adds information about word order into the embeddings.\n",
    "        - I am using sine and cosine functions of different frequencies to create positional encodings since it is a smooth periodic function. I chose this because it is better at avoiding overfitting,\n",
    "        and has the generalization ability that other alternatives like T5 and BERT lack. \n",
    "        - In Fetch rewards, a better choice would be to use BERT, which can be good for tasks like analyzing data/patterns over fixed timwe windows. \n",
    "        - d_model: embedding size (must be even for alternating sin-cos).\n",
    "        - max_len: max seqn length we expect.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # create a matrix of shape (max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # shape: (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000) / d_model))\n",
    "        \n",
    "        # apply sin to even indices and cosine to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # shape remains (max_len, d_model)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)  # store as a persistent buffer (not a parameter)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]  # make sure sequence length matches input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, num_heads=4, num_layers=2, dropout=0.1, max_len=50):\n",
    "        \"\"\"\n",
    "        A simple Sentence Transformer that encodes sentences into fixed-length vectors.\n",
    "        - vocab_size: no. of words in the vocabulary.\n",
    "        - d_model: dimensionality of embeddings and transformer hidden size.\n",
    "        - num_heads: no. of attention heads in the Transformer.\n",
    "        - num_layers: no. of Transformer encoder layers.\n",
    "        - dropout: dropout probability.\n",
    "        - max_len: max seqn length.\n",
    "        \"\"\"\n",
    "        super(SentenceTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)  # convert token indices to embeddings (batch_size, seq_len, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)  # add positional information\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        \n",
    "        # Pooling layer to reduce seqn length and get a fixed-size sentence embedding\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)  # mean pooling over token dimension\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass through the model. \"\"\"\n",
    "        x = self.embedding(x)  # convert token indices to dense vectors, shape: (batch_size, seq_len, d_model)\n",
    "        x = self.pos_encoder(x)  # add positional encoding\n",
    "        x = self.transformer_encoder(x)  # pass through transformer encoder\n",
    "        \n",
    "        # Pooling step\n",
    "        x = x.permute(0, 2, 1)  # Rearrange to (batch_size, d_model, seq_len) for pooling\n",
    "        x = self.pooling(x).squeeze(-1)  # mean pooling, shape: (batch_size, d_model)\n",
    "        return x  # output embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tokenizer(sentence, vocab, max_len=6):\n",
    "    \"\"\" \n",
    "    Tokenizes a sentence using NLTK and converts words to indices based on a vocabulary. \n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    token_ids = [vocab.get(word, vocab[\"<UNK>\"]) for word in tokens]\n",
    "    if len(token_ids) < max_len:\n",
    "        token_ids += [vocab[\"<PAD>\"]] * (max_len - len(token_ids))  # pad to max_len\n",
    "    return token_ids[:max_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Embeddings:\n",
      "Sentence: My dog asked for a raise, I told him to fetch his own paycheck\n",
      "Embedding: [-1.4645281  -0.02077602 -1.5391865   0.4521033   0.06101049 -0.8322439\n",
      "  0.23527747  0.6854346   0.3462597  -0.11951858 -0.04943123 -0.86029387\n",
      "  0.11193186  1.8133273  -1.2709274   2.0255535  -0.28470692 -1.2981143\n",
      "  0.749053   -0.49446544 -0.0879648   0.33344948 -0.9736144   0.06899364\n",
      " -0.5991185  -0.48273894  0.46165535  0.8682397  -0.11344402  0.82438844\n",
      "  1.7020359   0.5912024   0.41512123  0.04551455 -2.5492058   1.4014207\n",
      " -0.3305967   1.6340065   1.6474422  -0.23978777 -0.5385265  -0.6233191\n",
      "  0.300016   -0.825381    0.4184396   0.74974346  0.35828504 -0.03175083\n",
      " -0.51528144 -1.1523126   0.5120567   0.60272986 -1.456618    0.80301356\n",
      " -0.6090191   2.0761003  -0.30633292  0.8451187  -0.72727567  0.35224783\n",
      " -0.28926584  0.47845015  1.0099648   0.05782488 -0.6429448  -0.4509724\n",
      " -1.938521   -0.17681621 -0.6862107   0.43777862 -1.0125352   0.6615725\n",
      " -0.5226415   1.4313277   1.19824     0.5802271   1.4297365  -0.58333784\n",
      " -0.39915085 -0.08180352 -0.92039037 -0.09140453 -0.86479217  0.5224894\n",
      " -0.21753222  0.9385293  -1.1531463   0.3158674  -1.1355083   0.94916564\n",
      " -0.18281578 -0.63177043 -0.19259578  0.78468305  0.79315853  0.39750004\n",
      "  0.31370506 -0.25315914 -0.844882   -0.5688076  -0.6906209   0.44598392\n",
      "  0.0390871  -1.4046284   1.4064823  -0.10794881  1.3197716   0.6327704\n",
      " -1.2855221  -0.3292189  -1.5000849  -0.22718455 -0.7458818   0.6516507\n",
      " -0.31779423 -1.008192   -1.243024    1.4692391  -0.39186797 -1.1574112\n",
      " -0.05600727 -0.79376036 -0.03052448  0.71661144  0.08567952  1.6114922\n",
      "  0.9373211   0.41967404]\n",
      "Sentence: I threw a ball for my dog... he brought back a receipt\n",
      "Embedding: [-1.4672074  -0.31848538 -1.8399706   0.5426393   0.16737436 -0.9737713\n",
      "  0.3013727   0.16044867  0.28513405  0.2209283  -0.12484153 -0.9815192\n",
      "  0.15737467  2.3177445  -1.6965975   2.1405056  -0.49073517 -1.7217565\n",
      "  0.8322664  -0.53671324 -0.45558858  0.34647438 -1.206618   -0.11301709\n",
      " -0.71440345 -0.6434543   0.55349475  0.9982564  -0.1578597   0.5063532\n",
      "  1.8793602   0.78046376  0.30834037  0.14217071 -2.2127018   1.2602767\n",
      "  0.06525212  2.0750196   1.4068918  -0.05617034 -0.48813584 -0.6896863\n",
      "  0.18617319 -0.8491142   0.4605204   0.68369216  0.3650483  -0.18279405\n",
      " -0.55319273 -1.1060152   0.76607114  0.6209622  -1.4547138   0.8909989\n",
      " -0.62797743  2.3008096  -0.42498264  0.9624235  -0.81384426  0.7357622\n",
      "  0.08221693  0.6411155   0.6538148   0.4363972  -0.7963837  -0.75385547\n",
      " -1.8569015   0.27484962 -0.8409979   0.63917464 -0.8836668   0.6668429\n",
      " -0.5098672   1.3860469   1.5230722   0.4229723   1.5465155  -1.1313756\n",
      " -0.5107179   0.02412491 -1.3300927   0.06462339 -0.7025859   1.0556254\n",
      " -0.39691055  1.3327409  -1.2664934   0.43658125 -1.0815217   1.3009542\n",
      " -0.27774847 -0.95097595 -0.46003222  0.54922926  0.54013586  0.35712364\n",
      "  0.15093927 -0.4466881  -0.9906996  -0.4656075  -0.8926876   0.50957656\n",
      "  0.19767569 -1.3856945   1.2675722  -0.29651174  1.5643753   0.63379294\n",
      " -1.5706959  -0.3149766  -1.1106863   0.03264535 -0.5452033   0.6907808\n",
      " -0.441305   -1.2299987  -1.1115736   1.6451591  -0.43992892 -1.1966501\n",
      "  0.00632679 -1.0779209   0.26313797  0.79081744 -0.1528963   1.7164799\n",
      "  1.0378541   0.45982483]\n",
      "Sentence: Why don't dogs use phones? Too many collar ID issues\n",
      "Embedding: [-1.4210914  -0.26620123 -1.7535453   0.37998044  0.22181416 -0.9987114\n",
      "  0.37943685  0.47797254  0.14894286  0.20037822 -0.01724027 -0.88674617\n",
      "  0.1303928   2.1824656  -1.7845645   1.9270588  -0.51401776 -1.7127038\n",
      "  0.9259103  -0.68662673 -0.3643924   0.42595327 -1.1186901  -0.05145575\n",
      " -0.55074555 -0.49775505  0.5236246   1.213085   -0.24333934  0.76050615\n",
      "  1.9210795   0.7939043   0.25797188  0.09811281 -2.5415843   1.1274356\n",
      " -0.1645511   2.181289    1.126352   -0.47514185 -0.5947825  -0.8233423\n",
      "  0.21710338 -0.9732478   0.5013711   0.9445669   0.36469927 -0.27925873\n",
      " -0.43719962 -1.2947788   0.9183717   0.87514585 -1.4646978   0.7264357\n",
      " -0.6761276   2.0586357  -0.5197033   1.0865707  -0.75664574  0.34571996\n",
      "  0.12245142  0.56151015  0.94058585  0.18245585 -0.5592639  -0.8999041\n",
      " -2.164963    0.14413704 -0.6389864   0.66311306 -1.0170989   0.68719596\n",
      " -0.58451825  1.4308205   1.3382382   0.4545907   1.6052871  -1.0882869\n",
      " -0.4569036  -0.09772316 -1.3329884  -0.09116299 -0.5930881   0.9051998\n",
      " -0.30087635  1.0858663  -1.2310356   0.44089362 -1.0336572   1.1872098\n",
      " -0.29369906 -0.86615205 -0.12501088  0.8437686   0.82538885  0.44120607\n",
      "  0.08492911 -0.3734589  -0.8338127  -0.45691302 -0.69806     0.3616788\n",
      "  0.13560545 -1.4239916   1.3757871  -0.06914528  1.6229725   0.65649587\n",
      " -1.5906906  -0.29599372 -1.072925   -0.20709534 -0.4414327   0.64637244\n",
      " -0.398796   -1.3673178  -1.1302434   1.632988   -0.37619555 -0.95953506\n",
      "  0.00818281 -1.2788101   0.1209771   0.7585655   0.15061556  1.9303917\n",
      "  0.8549381   0.57591903]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siddu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000  # Assume a vocabulary of 10k words\n",
    "d_model = 128\n",
    "sentence_transformer = SentenceTransformer(vocab_size, d_model)\n",
    "\n",
    "# Dummy vocabulary\n",
    "dummy_vocab = {\n",
    "    \"<PAD>\": 0, \"<UNK>\": 1, \"fetch\": 2, \"rewards\": 3, \"dog\": 4, \"loves\": 5,\n",
    "    \"treats\": 6, \"and\": 7, \"belly\": 8, \"rubs\": 9, \"woof\": 10\n",
    "}\n",
    "\n",
    "# Sample sentences\n",
    "sample_sentences = [\n",
    "    \"My dog asked for a raise, I told him to fetch his own paycheck\",\n",
    "    \"I threw a ball for my dog... he brought back a receipt\",\n",
    "    \"Why don't dogs use phones? Too many collar ID issues\"\n",
    "]\n",
    "\n",
    "# Convert sentences to token indices\n",
    "tokenized_inputs = [nltk_tokenizer(sent, dummy_vocab) for sent in sample_sentences]\n",
    "input_tensor = torch.tensor(tokenized_inputs)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "# Run the sentences through the model\n",
    "embeddings = sentence_transformer(input_tensor)\n",
    "\n",
    "# Print sentence embeddings\n",
    "print(\"Sentence Embeddings:\")\n",
    "for i, sent in enumerate(sample_sentences):\n",
    "    print(f\"Sentence: {sent}\")\n",
    "    print(f\"Embedding: {embeddings[i].detach().numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Design Choices Outside the Transformer Backbone\n",
    "\n",
    "For Task 1, I made several architectural choices outside the transformer backbone to keep the model simple, efficient, and easy to run. I also made it generalizable since I don't exactly know have a target dataset\n",
    "\n",
    "##### 1. Token Embedding Layer (nn.Embedding)\n",
    "- Choice: Used nn.Embedding(vocab_size, d_model), mapping each word index to a d_model-dimensional vector.  \n",
    "- Alternative:  \n",
    "  - Could have used pretrained word embeddings (e.g., GloVe, Word2Vec) for better initialization.  \n",
    "  - Opted for learnable embeddings to let the model adapt to the dataset.  \n",
    "\n",
    "##### 2. Positional Encoding\n",
    "- Choice: Used a sine-cosine positional encoding to preserve sequence structure.  \n",
    "- Alternative:  \n",
    "  - Could have used learned positional embeddings (like BERT), but sine-cosine encoding allows better generalization to longer sequences.  \n",
    "\n",
    "##### 3. Sentence Pooling (nn.AdaptiveAvgPool1d)\n",
    "- Choice: Used mean pooling, averaging token embeddings to create a fixed-size representation of the sentence.  \n",
    "- Alternative:  \n",
    "  - CLS Token Approach (as in BERT) – Uses a special [CLS] token representation as the sentence embedding.  \n",
    "  - Max Pooling – Selects the most activated token feature instead of averaging.  \n",
    "\n",
    "##### 4. Vocabulary Handling (<UNK> and <PAD> Tokens)\n",
    "- Choice:  \n",
    "  - <UNK> (Unknown Token): Replaces words not in the vocabulary.  \n",
    "  - <PAD> (Padding Token): Ensures uniform sequence lengths by padding shorter sentences.  \n",
    "\n",
    "\n",
    "##### 5. Fixed max_len=6 for Token Sequences\n",
    "- Choice: Set max_len=6 for batch processing, meaning sentences longer than 6 tokens are truncated, and shorter ones are padded.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "- I will expand the Sentence Transformer to  handle a multi-task learning setting. This will allow the model to handle:\n",
    "    - Task A: Sentence Classification\n",
    "    - Task B: Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskSentenceTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, num_heads=4, num_layers=2, dropout=0.1, max_len=50, num_classes_A=3):\n",
    "        \"\"\"\n",
    "        Multi-Task Sentence Transformer that encodes sentences into fixed-length vectors and supports multiple tasks.\n",
    "        - vocab_size: no. of words in the vocabulary.\n",
    "        - d_model: dimensionality of embeddings and transformer hidden size.\n",
    "        - num_heads: no. of attention heads in the Transformer.\n",
    "        - num_layers: no. of Transformer encoder layers.\n",
    "        - dropout: dropout probability.\n",
    "        - max_len: max seqn length.\n",
    "        - num_classes_A: no. of output classes for Task A (sentence classification).\n",
    "        - num_classes_B: no. of output classes for Task B (sentiment analysis).\n",
    "        \"\"\"\n",
    "        super(MultiTaskSentenceTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)  # Convert token indices to embeddings (batch_size, seq_len, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)  # Add positional information\n",
    "\n",
    "        # transformr encoder layers\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        \n",
    "        # Pooling layer to reduce seqn length and get a fixed-size sentence embedding\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)  # Mean pooling over token dimension\n",
    "        \n",
    "        # Task A: Sentence Classification Head\n",
    "        self.task_A_head = nn.Linear(d_model, num_classes_A)  # fully connected layer for classification\n",
    "        \n",
    "        # Task B: Sentiment Analysis Head\n",
    "        self.task_B_head = nn.Linear(d_model, 1)  # output a single sentiment score between -1 and 1\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "\n",
    "        \"\"\" Initialize weights using Kaiming He initialization for better convergence with ReLU. \"\"\"\n",
    "        for param in self.parameters():\n",
    "            if param.dim() > 1:\n",
    "                nn.init.kaiming_uniform_(param, nonlinearity='relu')\n",
    "            elif param.dim() == 1:  # ensure biases are not all zeros\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        - x: Input token indices (batch_size, seq_len).\n",
    "        Returns:\n",
    "        - task_A_output: Sentence classification logits (batch_size, num_classes_A).\n",
    "        - task_B_output: Sentiment classification logits (batch_size, num_classes_B).\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)  # Convert token indices to dense vectors, shape: (batch_size, seq_len, d_model)\n",
    "        x = self.pos_encoder(x)  # Add positional encoding, shape remains (batch_size, seq_len, d_model)\n",
    "        x = self.transformer_encoder(x)  # Pass through transformer encoder, shape remains (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Pooling step\n",
    "        x = x.permute(0, 2, 1)  # Rearrange to (batch_size, d_model, seq_len) for pooling\n",
    "        x = self.pooling(x).squeeze(-1)  # Mean pooling, shape: (batch_size, d_model)\n",
    "        \n",
    "        # Separate outputs for each task\n",
    "        task_A_output = self.task_A_head(x)  # Sentence Classification, shape: (batch_size, num_classes_A)\n",
    "        task_B_output = torch.tanh(self.task_B_head(x))  # Sentiment score between -1 and 1\n",
    "        \n",
    "        # Convert logits to class predictions\n",
    "        task_A_prediction = torch.argmax(task_A_output, dim=1)  # Predicted class index for sentence classification\n",
    "        task_B_prediction = torch.argmax(task_B_output, dim=1)  # Predicted class index for sentiment analysis\n",
    "        \n",
    "        return task_A_prediction, task_B_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changes Made to the Architecture in Task 2 from Task 1 to Support Multi-Task Learning\n",
    "\n",
    "In Task 2, I expanded the sentence transformer model to support multi-task learning, allowing it to handle both sentence classification and sentiment analysis simultaneously. The modifications made were:\n",
    "\n",
    "##### 1. Addition of Multiple Task-Specific Heads  \n",
    "- Task 1 had a single output head for generating fixed-length sentence embeddings.  \n",
    "- Task 2 introduces two separate fully connected layers:  \n",
    "  - `task_A_head`: A linear layer for sentence classification that outputs logits corresponding to predefined categories.  \n",
    "  - `task_B_head`: A linear layer that outputs a single continuous value representing sentiment, scaled between -1 (negative) and 1 (positive) using torch.tanh().  \n",
    "\n",
    "##### 2. Shared Transformer Backbone  \n",
    "- I use the same transformer encoder and positional encoding. \n",
    "\n",
    "#### 3. Weight Initialization  \n",
    "- I initialized weigfhts with `Kaiming He initialization`, which is optimized for deep networks using ReLU activations.  \n",
    "- I chose Kaiming He over other options like Xavier and Gaussian because Kaiming He can be optimized for ReLU activation\n",
    "- Bias parameters are initialized to zeros to prevent biases from affecting early learning dynamics.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "#### 1. Freezing the Entire Network  \n",
    "- **Implications:**\n",
    "  - The model doesn't update any of its parameters during training. So we use frozen networks when adapting pre-trained transformers to a limited data for which the embeddings are already optimized.  \n",
    "  - In the case of Fetch rewards, you would use a pre-trained model directly on new customer data without any training. This could be either message classification (what the review is about), sentimental analysis, or other tasks.  \n",
    "- **Advantages:**  \n",
    "  - When there is little data, training the model again on this could lead to overfitting. Freezing the network solves this issue! \n",
    "  - Reduces computational cost and time.\n",
    "\n",
    "#### 2. Freezing Only the Transformer Backbone  \n",
    "- **Implications:**  \n",
    "  - The transformer layers would be frozen, but we train the task-specific heads again on new data and update their paramaters. \n",
    "  - For instance, if Fetch already has a pre-trained model that classifies user reviews based on the type of messages, you could add another classification criteria. So we can add a new classification task to classify these reviews based on the type of the product it's. To do this, only the classification heads have to be trained. \n",
    "- **Advantages:**  \n",
    "  - Reduces the computational time and cost and the number of trainable parameters. \n",
    "  - Uses the representations from pretraining while still adapting to the new task.  \n",
    "\n",
    "#### 3. Freezing One of the Task-Specific Heads  \n",
    "- **Implications:**  \n",
    "  - One of the tasks will rely entirely on pre-trained weights, while the other task learns new task-specific features.  \n",
    "  - For instance, if a model classifies sentences well but doesn't predict the sentiment good enough for certain sentences, we can free the classification head and train the sentimental analysis head. \n",
    "- **Advantages:**  \n",
    "  - Useful if one task generalizes well and does not need additional fine-tuning while focusing learning on the second task.  \n",
    "  - Avoids overfitting on one of the tasks while optimizing the other. \n",
    "\n",
    "---\n",
    "\n",
    "#### Transfer Learning\n",
    "\n",
    "Considering a scenario at Fetch rewards where transfer learning would be beneficial:\n",
    "We want to analyze customer feedback on receipts to detect purchase satisfaction and classify receipt categories \n",
    "\n",
    "#### 1. Choosing a Pre-Trained Model  \n",
    "- BERT, RoBERTa, or T5 would be good choices as they have been pre-trained on large language corpora and can be fine-tuned for specific NLP tasks.  \n",
    "- I would uise RoBERTa as it works well with analyzing sentiment (brand satisfaction in this case), and also because I am more familiar with it, having used it before. \n",
    "\n",
    "#### 2. Layers to Freeze/Unfreeze  \n",
    "- RoBERTa has 12 layers, first 9 for language representation and the last 3 for task specific patterns. I would choose the number of layers to freeze depending on the size of the dataset.\n",
    "- Smaller the dataset, more the layers that should be frozen. If it's a small dataset, I would free the last 3 (task dependent) layers to prevent overfitting. If the dataset if very big or if the data is different from what the model was pre-trained on, I would freeze 4 or less layers. \n",
    "- If I have enough time, I'd just check validation accuracy vs number of frozen layers in [3, 4, 6, 9] to confirm my intuition.\n",
    "\n",
    "#### 3. Rationale Behind These Choices  \n",
    "- RoBERTa is ideal for sentiment analysis and receipt classification due to its strong contextual understanding. Freezing more layers (e.g., first 9) retains general language knowledge for small datasets, while freezing fewer layers (e.g., 3-6) allows adaptation to data-specific terminology if the dataset is large or domain-specific. I would start by freezing 9 layers and fine-tune only the last 3, adjusting based on validation accuracy and overfitting trends.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "num_classes_A = 4\n",
    "d_model = 128\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "model = MultiTaskSentenceTransformer(vocab_size, d_model, num_classes_A=num_classes_A)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion_A = nn.CrossEntropyLoss()\n",
    "criterion_B = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = None #hypothetical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_loss_A, total_loss_B = 0, 0\n",
    "    \n",
    "    for inputs, labels_A, labels_B in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        task_A_output, task_B_output = model(inputs)\n",
    "        \n",
    "        # Compute losses\n",
    "        loss_A = criterion_A(task_A_output, labels_A)\n",
    "        loss_B = criterion_B(task_B_output.squeeze(), labels_B)\n",
    "        \n",
    "        # Backpropagation\n",
    "        total_loss = loss_A + loss_B\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss_A += loss_A.item()\n",
    "        total_loss_B += loss_B.item()\n",
    "    \n",
    "    print(f\"epoch {epoch+1}: Loss A = {total_loss_A/len(dataloader)}, Loss B= {total_loss_B/len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
